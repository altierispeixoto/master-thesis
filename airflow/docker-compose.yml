version: '2.1'
services:
    redis:
        image: 'redis:5.0.5'
        command: redis-server --requirepass redispass
        networks:
            airflow-network:
                ipv4_address: 10.5.0.2
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        volumes:
            - ./db-data:/var/lib/postgresql/data
        ports:
            - "5432:5432"
        networks:
            airflow-network:
                ipv4_address: 10.5.0.3
    webserver:
        build: .
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - EXECUTOR=Celery
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - REDIS_PASSWORD=redispass
        volumes:
            - ./dags:/usr/local/airflow/dags
            # Uncomment to include custom plugins
            - ./plugins:/usr/local/airflow/plugins
            - /work/datalake:/usr/local/airflow/data/
            - ./neo4j:/usr/local/airflow/neo4j
            -  /var/run/docker.sock:/var/run/docker.sock:ro
        ports:
            - "8989:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
        networks:
            airflow-network:
                ipv4_address: 10.5.0.4
    flower:
        #image: puckel/docker-airflow:1.10.4
        build: .
        restart: always
        depends_on:
            - redis
        environment:
            - EXECUTOR=Celery
            - REDIS_PASSWORD=redispass
        ports:
            - "5555:5555"
        command: flower
        networks:
            airflow-network:
                ipv4_address: 10.5.0.5
    scheduler:
        #image: puckel/docker-airflow:1.10.4
        build: .
        restart: always
        depends_on:
            - webserver
        volumes:
            - ./dags:/usr/local/airflow/dags
            # Uncomment to include custom plugins
            - ./plugins:/usr/local/airflow/plugins
        environment:
            - LOAD_EX=n
            - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - EXECUTOR=Celery
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - REDIS_PASSWORD=redispass
        command: scheduler
        networks:
            airflow-network:
                ipv4_address: 10.5.0.6
    worker:
        #image: puckel/docker-airflow:1.10.4
        build: .
        restart: always
        depends_on:
            - scheduler
        volumes:
            - ./dags:/usr/local/airflow/dags
            # Uncomment to include custom plugins
            - ./plugins:/usr/local/airflow/plugins
            -  /work/datalake/:/usr/local/airflow/data
            - ./neo4j:/usr/local/airflow/neo4j
            -  /var/run/docker.sock:/var/run/docker.sock:ro
        environment:
            - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - EXECUTOR=Celery
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - REDIS_PASSWORD=redispass
        command: worker
        networks:
            airflow-network:
                ipv4_address: 10.5.0.7
#    metabase-app:
#        image: metabase/metabase
#        restart: always
#        ports:
#            - 3001:3000
#        volumes:
#            # declare your mount volume /host/dir:/container/dir
#            - /metabase-data:/metabase-data
#        environment:
#            MB_DB_TYPE: postgres
#            MB_DB_DBNAME: metabase
#            MB_DB_PORT: 5432
#            MB_DB_USER: airflow
#            MB_DB_PASS: airflow
#            MB_DB_HOST: postgres
#        depends_on:
#            - postgres
#        links:
#            - postgres
#        networks:
#            airflow-network:
#                ipv4_address: 10.5.0.8
#    neo4j:
#        image: neo4j:3.5.6
#        restart: always
#        ports:
#          - 7474:7474
#          - 7687:7687
#        volumes:
#          - ./neo4j/data:/data
#          - ./neo4j/logs:/logs
#          - ./neo4j/plugins:/plugins
#          - ./neo4j/import:/import
#        environment:
#            NEO4J_dbms_security_procedures_unrestricted: algo.*, apoc.*
#            NEO4J_apoc_import_file_enabled: "true"
#            NEO4J_dbms_memory_pagecache_size: "8"
#            NEO4J_dbms_memory_heap_initial__size: "10G"
#            NEO4J_dbms_memory_heap_max__size: "10G"
#        networks:
#            airflow-network:
#                ipv4_address: 10.5.0.9
networks:
  airflow-network:
    driver: bridge
    ipam:
     config:
       - subnet: 10.5.0.0/16
         gateway: 10.5.0.1


#    spark-master:
#        image: altr/spark
#        container_name: spark-master
#        ports:
#            - "8088:8080"
#            - "7077:7077"
#        environment:
#            - INIT_DAEMON_STEP=setup_spark
#        volumes:
#            - ./spark-urbs-processing:/spark-urbs-processing
#            - ./data/raw:/data/raw
#            - ./data/processed/:/data/processed
#    spark-worker-1:
#        image: altr/spark
#        container_name: spark-worker-1
#        depends_on:
#            - spark-master
#        ports:
#            - "8089:8081"
#        environment:
#            - "SPARK_MASTER=spark://spark-master:7077"
#        volumes:
#            - ./spark-urbs-processing:/spark-urbs-processing
#            - ./data/raw:/data/raw
#            - ./data/processed/:/data/processed

#    spark-master:
#        image: bde2020/spark-master:2.4.4-hadoop2.7
#        container_name: spark-master
#        ports:
#            - "8088:8080"
#            - "7077:7077"
#        environment:
#            - INIT_DAEMON_STEP=setup_spark
#        volumes:
#            - ./spark-urbs-processing:/spark-urbs-processing
#            - ./data/raw:/data/raw
#            - ./data/processed/:/data/processed
#    spark-worker-1:
#        image: bde2020/spark-worker:2.4.4-hadoop2.7
#        container_name: spark-worker-1
#        depends_on:
#            - spark-master
#        ports:
#            - "8089:8081"
#        environment:
#            - "SPARK_MASTER=spark://spark-master:7077"
#        volumes:
#            - ./spark-urbs-processing:/spark-urbs-processing
#            - ./data/raw:/data/raw
#            - ./data/processed/:/data/processed
